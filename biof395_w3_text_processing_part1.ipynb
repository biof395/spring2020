{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biof395_w3_text_processing_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfADtNY3BDz2",
        "colab_type": "text"
      },
      "source": [
        "# Sample data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsSY3JZbYi6",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah7zV0rGBIg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for demonstration purpose, we take the abstract from a pubmed article \n",
        "#(PMID: 30443819) and apply text processing\n",
        "\n",
        "document = \"The curation of neuroscience entities is crucial to ongoing efforts in neuroinformatics and computational neuroscience, such as those being deployed in the context of continuing large-scale brain modelling projects. However, manually sifting through thousands of articles for new information about modelled entities is a painstaking and low-reward task. Text mining can be used to help a curator extract relevant information from this literature in a systematic way. We propose the application of text mining methods for the neuroscience literature. Specifically, two computational neuroscientists annotated a corpus of entities pertinent to neuroscience using active learning techniques to enable swift, targeted annotation. We then trained machine learning models to recognise the entities that have been identified. The entities covered are Neuron Types, Brain Regions, Experimental Values, Units, Ion Currents, Channels, and Conductances and Model organisms. We tested a traditional rule-based approach, a conditional random field and a model using deep learning named entity recognition, finding that the deep learning model was superior. Our final results show that we can detect a range of named entities of interest to the neuroscientist with a macro average precision, recall and F1 score of 0.866, 0.817 and 0.837 respectively. The contributions of this work are as follows: 1) We provide a set of Named Entity Recognition (NER) tools that are capable of detecting neuroscience entities with performance above or similar to prior work. 2) We propose a methodology for training NER tools for neuroscience that requires very little training data to get strong performance. This can be adapted for any sub-domain within neuroscience. 3) We provide a small corpus with annotations for multiple entity types, as well as annotation guidelines to help others reproduce our experiments.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xATDPFhPBRf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(document))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL4QVlvUbcI6",
        "colab_type": "text"
      },
      "source": [
        "# Download packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa1drhA2ErHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jyQbqTz6f2_",
        "colab_type": "text"
      },
      "source": [
        "# Case folding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umONPAFj7xNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_lower = document.lower()\n",
        "print(document_lower)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYALEmkJBmCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_upper = document.upper()\n",
        "print(document_upper)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5zIUqxsBvkV",
        "colab_type": "text"
      },
      "source": [
        "Note that you can lower/upper at sentence or word level as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUSPh1HsB8a9",
        "colab_type": "text"
      },
      "source": [
        "# Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVSpxfFmDkS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_split = document_lower.split()\n",
        "for word in words_split:\n",
        "  print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzDhNIJgDxme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "words_nltk = word_tokenize(document_lower)\n",
        "\n",
        "for word in words_nltk:\n",
        "  print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcffYOjHGPly",
        "colab_type": "text"
      },
      "source": [
        "# Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ-MAWavGX--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_split = document_lower.split('.')\n",
        "\n",
        "for sentence in sentences_split:\n",
        "  print(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYEnqk3lHEy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "sentences_nltk = sent_tokenize(document_lower)\n",
        "for sentence in sentences_nltk:\n",
        "  print(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QdJNhfcl4Qo",
        "colab_type": "text"
      },
      "source": [
        "From now, we can have a very simple case study on analyzing word frequency of the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsPlLv4iereb",
        "colab_type": "text"
      },
      "source": [
        "# Word frequency analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfUetwCLevZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = {}\n",
        "\n",
        "#compute word frequency on the raw document\n",
        "words_nltk = word_tokenize(document)\n",
        "\n",
        "for word in words_nltk:\n",
        "  if word not in word_freq:\n",
        "    word_freq[word] = 1\n",
        "  else:\n",
        "    word_freq[word] += 1\n",
        "\n",
        "from operator import itemgetter\n",
        "freqs = sorted(word_freq.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "for freq in freqs:\n",
        "  print(freq)\n",
        "\n",
        "print('total number of tokens', len(word_freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG1txEsClUGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = {}\n",
        "\n",
        "#compute word frequency on the raw document & case folding\n",
        "words_nltk = word_tokenize(document_lower)\n",
        "\n",
        "for word in words_nltk:\n",
        "  if word not in word_freq:\n",
        "    word_freq[word] = 1\n",
        "  else:\n",
        "    word_freq[word] += 1\n",
        "\n",
        "from operator import itemgetter\n",
        "freqs = sorted(word_freq.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "for freq in freqs:\n",
        "  print(freq)\n",
        "\n",
        "print('total number of tokens', len(word_freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5SyCX0ygx7z",
        "colab_type": "text"
      },
      "source": [
        "# Removing puntuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clHp9cVPhxvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh_1zriuh3zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = {}\n",
        "\n",
        "#compute word frequency on the raw document & case folding & removing punctuations\n",
        "words_nltk = word_tokenize(document_lower)\n",
        "for word in words_nltk:\n",
        "  if word not in punctuation:\n",
        "    if word not in word_freq:\n",
        "      word_freq[word] = 1\n",
        "    else:\n",
        "      word_freq[word] += 1\n",
        "\n",
        "from operator import itemgetter\n",
        "freqs = sorted(word_freq.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "for freq in freqs:\n",
        "  print(freq)\n",
        "\n",
        "print('total number of tokens', len(word_freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhOpooN3jzU5",
        "colab_type": "text"
      },
      "source": [
        "# Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zsNCas7j3ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopword_list = set(stopwords.words('english'))\n",
        "print('number of stopword list', len(stopword_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn95Z2N0mZM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = {}\n",
        "\n",
        "#compute word frequency on the raw document & case folding & removing punctuations & removing stopwords\n",
        "words_nltk = word_tokenize(document_lower)\n",
        "for word in words_nltk:\n",
        "  if word not in punctuation and word not in stopword_list:\n",
        "    if word not in word_freq:\n",
        "      word_freq[word] = 1\n",
        "    else:\n",
        "      word_freq[word] += 1\n",
        "\n",
        "from operator import itemgetter\n",
        "freqs = sorted(word_freq.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "for freq in freqs:\n",
        "  print(freq)\n",
        "\n",
        "print('total number of tokens', len(word_freq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5SyjJY3pNc0",
        "colab_type": "text"
      },
      "source": [
        "# Stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRionN8dpQql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "example_words = ['demonstrate', 'demonstration', 'demonstrating']\n",
        "\n",
        "for example_word in example_words:\n",
        "  print(example_word, 'is stemmed to', stemmer.stem(example_word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwK1ZLO3q21K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_freq = {}\n",
        "\n",
        "#compute word frequency on the raw document \n",
        "#& case folding & removing punctuations & removing stopwords\n",
        "#stemming\n",
        "words_nltk = word_tokenize(document_lower)\n",
        "for word in words_nltk:\n",
        "  if word not in punctuation and word not in stopword_list:\n",
        "    word = stemmer.stem(word)\n",
        "    if word not in word_freq:\n",
        "      word_freq[word] = 1\n",
        "    else:\n",
        "      word_freq[word] += 1\n",
        "\n",
        "from operator import itemgetter\n",
        "freqs = sorted(word_freq.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "for freq in freqs:\n",
        "  print(freq)\n",
        "\n",
        "print('total number of tokens', len(word_freq))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}