{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bio395_w4_machine_learning_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umOCEHObLZOR",
        "colab_type": "text"
      },
      "source": [
        "#Load a dataset from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTejTn52G48t",
        "colab_type": "code",
        "outputId": "3c64e531-6845-47c6-fecf-9d9dce855bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "print(iris_dataset.DESCR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2n0O9HeM51C",
        "colab_type": "text"
      },
      "source": [
        "# Explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDC37NbpM96H",
        "colab_type": "code",
        "outputId": "39a4392d-50c6-46e3-f49a-175fb937aa8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "print('feature names', iris_dataset.feature_names)\n",
        "print('label names', iris_dataset.target_names)\n",
        "\n",
        "#get features and labels\n",
        "features = iris_dataset['data']\n",
        "labels = iris_dataset['target']\n",
        "\n",
        "print('feature shape', features.shape)\n",
        "print('label shape', labels.shape)\n",
        "\n",
        "print('the features of the first 5 instances:')\n",
        "print(features[:5])\n",
        "\n",
        "print('the labels of the first 5 instances:')\n",
        "print(labels[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "label names ['setosa' 'versicolor' 'virginica']\n",
            "feature shape (150, 4)\n",
            "label shape (150,)\n",
            "the features of the first 5 instances:\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "the labels of the first 5 instances:\n",
            "[0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIfZ4SayOG1h",
        "colab_type": "text"
      },
      "source": [
        "# Train-valid-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8cwFseyOMJQ",
        "colab_type": "code",
        "outputId": "62db328d-f35f-40a5-971e-2783d3ed4aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train_set_size = int(len(features)*0.7)\n",
        "valid_set_size = int(len(features)*0.1)\n",
        "test_set_size = int(len(features)*0.2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainvalid, X_test, y_trainvalid, y_test = train_test_split(features, labels, test_size=test_set_size, random_state=11)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_trainvalid, y_trainvalid, test_size=valid_set_size, random_state=11)\n",
        "\n",
        "print('training set size', len(X_train))\n",
        "print('valid set size', len(X_valid))\n",
        "print('test set size', len(X_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set size 105\n",
            "valid set size 15\n",
            "test set size 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKvC6OfeRaqg",
        "colab_type": "text"
      },
      "source": [
        "# Training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-f1Z0-iSSif",
        "colab_type": "text"
      },
      "source": [
        "Recall that we use the training set for training the model.\n",
        "\n",
        "We use the validation set for tuning the hyperparameters.\n",
        "\n",
        "When the model is finalized, we apply the model to the test set and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-FmOgRSmUm",
        "colab_type": "code",
        "outputId": "b9f30f50-8aca-4be0-df11-536e2fa73d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#we will talk about different models in the following weeks\n",
        "\n",
        "#create an instance of the RidgeClassifier model \n",
        "model = RidgeClassifier(random_state=11)\n",
        "print(model)\n",
        "\n",
        "#use the training set to train the model \n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#how is the performance on the validation set?\n",
        "\n",
        "y_predict_valid = model.predict(X_valid)\n",
        "print ('accuracy in the validation set', accuracy_score(y_valid, y_predict_valid))\n",
        "\n",
        "#how is the performance on the test set?\n",
        "y_predict_test = model.predict(X_test)\n",
        "print ('accuracy in the test set', accuracy_score(y_test, y_predict_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=11, solver='auto',\n",
            "                tol=0.001)\n",
            "accuracy in the validation set 0.8666666666666667\n",
            "accuracy in the test set 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grwNEV6DdS4W",
        "colab_type": "text"
      },
      "source": [
        "The accuracy of the test set is significantly lower than the accuracy of the validation set. What to do then? \n",
        "\n",
        "Common ways are (1) tuning parameters, (2) adding regularizations, (3) analyzing errors and (4) changing to different models.\n",
        "We will go through these in much more depth in the following weeks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea_FIuDXbO3V",
        "colab_type": "text"
      },
      "source": [
        "# Bag-of-word models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpanDpILbS6l",
        "colab_type": "code",
        "outputId": "b0bbf1ff-b4d7-4931-8f46-396c633a2b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "docs = ['this is a general news', 'this is a sports news', 'this is another news on sports', 'these news examples are so boring']\n",
        "\n",
        "for doc in docs:\n",
        "  print(doc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is a general news\n",
            "this is a sports news\n",
            "this is another news on sports\n",
            "these news examples are so boring\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixM7_mB1fCL-",
        "colab_type": "code",
        "outputId": "6cc7d0d3-a7dd-45dc-b37e-b3af2cb1ea2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#you can do preprocessing first and then pass the processed texts to CountVectorizer\n",
        "\n",
        "freq_vectorizer = CountVectorizer(stop_words='english')\n",
        "freq_vectors = freq_vectorizer.fit_transform(docs)\n",
        "\n",
        "print(freq_vectorizer.get_feature_names())\n",
        "print(freq_vectors.toarray())\n",
        "\n",
        "freqs = pd.DataFrame(data=freq_vectors.toarray(), columns=freq_vectorizer.get_feature_names())\n",
        "\n",
        "print(freqs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['boring', 'examples', 'general', 'news', 'sports']\n",
            "[[0 0 1 1 0]\n",
            " [0 0 0 1 1]\n",
            " [0 0 0 1 1]\n",
            " [1 1 0 1 0]]\n",
            "   boring  examples  general  news  sports\n",
            "0       0         0        1     1       0\n",
            "1       0         0        0     1       1\n",
            "2       0         0        0     1       1\n",
            "3       1         1        0     1       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_cPOXskhygn",
        "colab_type": "text"
      },
      "source": [
        "The word 'news' is not a stopword, but it occurs in every document in the collection. Is it informative at all?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc8qsfsWh-Kc",
        "colab_type": "code",
        "outputId": "05149b87-abfc-4983-ac1c-a574d2ee27ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#note that we often use normalization in this step. Here I set norm to None for demonstration purpose\n",
        "#there are many vairations of tf-idfs, which are all reasonable in practice.\n",
        "#please see the spec in https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', norm=None)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(docs)\n",
        "\n",
        "tfidfs = pd.DataFrame(data=tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "print(tfidfs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     boring  examples   general  news    sports\n",
            "0  0.000000  0.000000  1.916291   1.0  0.000000\n",
            "1  0.000000  0.000000  0.000000   1.0  1.510826\n",
            "2  0.000000  0.000000  0.000000   1.0  1.510826\n",
            "3  1.916291  1.916291  0.000000   1.0  0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}